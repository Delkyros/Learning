{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This project involves creating a classifier that categorizes news from the hypothetical newspaper \"One News\". \n",
    "\n",
    "Two techniques are covered: one hot encode, which represents words as sparse vectors with only one position being 1, and word2vec, which uses dense vectors of fixed size to represent words, enabling generalization and contextual understanding.\n",
    "\n",
    "Word2vec is trained in two ways: continuous bag of words (CBOW), which tries to predict the word based on the context, and skip gram, which tries to predict the context given the word. The model is developed using the Gensim library in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "article_train = pd.read_csv('https://caelum-online-public.s3.amazonaws.com/1638-word-embedding/treino.csv')\n",
    "article_test = pd.read_csv('https://caelum-online-public.s3.amazonaws.com/1638-word-embedding/teste.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Após polêmica, Marine Le Pen diz que abomina n...</td>\n",
       "      <td>A candidata da direita nacionalista à Presidên...</td>\n",
       "      <td>2017-04-28</td>\n",
       "      <td>mundo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mundo/2017/04/187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Macron e Le Pen vão ao 2º turno na França, em ...</td>\n",
       "      <td>O centrista independente Emmanuel Macron e a d...</td>\n",
       "      <td>2017-04-23</td>\n",
       "      <td>mundo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mundo/2017/04/187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apesar de larga vitória nas legislativas, Macr...</td>\n",
       "      <td>As eleições legislativas deste domingo (19) na...</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>mundo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mundo/2017/06/189...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Governo antecipa balanço, e Alckmin anuncia qu...</td>\n",
       "      <td>O número de ocorrências de homicídios dolosos ...</td>\n",
       "      <td>2015-07-24</td>\n",
       "      <td>cotidiano</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/cotidiano/2015/07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Após queda em maio, a atividade econômica sobe...</td>\n",
       "      <td>A economia cresceu 0,25% no segundo trimestre,...</td>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>mercado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mercado/2017/08/1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Após polêmica, Marine Le Pen diz que abomina n...   \n",
       "1  Macron e Le Pen vão ao 2º turno na França, em ...   \n",
       "2  Apesar de larga vitória nas legislativas, Macr...   \n",
       "3  Governo antecipa balanço, e Alckmin anuncia qu...   \n",
       "4  Após queda em maio, a atividade econômica sobe...   \n",
       "\n",
       "                                                text        date   category  \\\n",
       "0  A candidata da direita nacionalista à Presidên...  2017-04-28      mundo   \n",
       "1  O centrista independente Emmanuel Macron e a d...  2017-04-23      mundo   \n",
       "2  As eleições legislativas deste domingo (19) na...  2017-06-19      mundo   \n",
       "3  O número de ocorrências de homicídios dolosos ...  2015-07-24  cotidiano   \n",
       "4  A economia cresceu 0,25% no segundo trimestre,...  2017-08-17    mercado   \n",
       "\n",
       "  subcategory                                               link  \n",
       "0         NaN  http://www1.folha.uol.com.br/mundo/2017/04/187...  \n",
       "1         NaN  http://www1.folha.uol.com.br/mundo/2017/04/187...  \n",
       "2         NaN  http://www1.folha.uol.com.br/mundo/2017/06/189...  \n",
       "3         NaN  http://www1.folha.uol.com.br/cotidiano/2015/07...  \n",
       "4         NaN  http://www1.folha.uol.com.br/mercado/2017/08/1...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grandes irmãos</td>\n",
       "      <td>RIO DE JANEIRO - O Brasil, cada vez menos famí...</td>\n",
       "      <td>2017-03-06</td>\n",
       "      <td>colunas</td>\n",
       "      <td>ruycastro</td>\n",
       "      <td>http://www1.folha.uol.com.br/colunas/ruycastro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Haddad congela orçamento e suspende emendas de...</td>\n",
       "      <td>O prefeito de São Paulo, Fernando Haddad (PT),...</td>\n",
       "      <td>2016-08-10</td>\n",
       "      <td>colunas</td>\n",
       "      <td>monicabergamo</td>\n",
       "      <td>http://www1.folha.uol.com.br/colunas/monicaber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Proposta de reforma da Fifa tem a divulgação d...</td>\n",
       "      <td>A Fifa divulgou, nesta quinta (10), um relatór...</td>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>esporte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/esporte/2015/09/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mercado incipiente, internet das coisas conect...</td>\n",
       "      <td>Bueiros, coleiras, aparelhos hospitalares, ele...</td>\n",
       "      <td>2016-11-09</td>\n",
       "      <td>mercado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mercado/2016/09/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mortes: Psicanalista, estudou o autismo em cri...</td>\n",
       "      <td>Toda vez que o grupo de amigos de Silvana Rabe...</td>\n",
       "      <td>2017-02-07</td>\n",
       "      <td>cotidiano</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/cotidiano/2017/07...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                     Grandes irmãos   \n",
       "1  Haddad congela orçamento e suspende emendas de...   \n",
       "2  Proposta de reforma da Fifa tem a divulgação d...   \n",
       "3  Mercado incipiente, internet das coisas conect...   \n",
       "4  Mortes: Psicanalista, estudou o autismo em cri...   \n",
       "\n",
       "                                                text        date   category  \\\n",
       "0  RIO DE JANEIRO - O Brasil, cada vez menos famí...  2017-03-06    colunas   \n",
       "1  O prefeito de São Paulo, Fernando Haddad (PT),...  2016-08-10    colunas   \n",
       "2  A Fifa divulgou, nesta quinta (10), um relatór...  2015-10-09    esporte   \n",
       "3  Bueiros, coleiras, aparelhos hospitalares, ele...  2016-11-09    mercado   \n",
       "4  Toda vez que o grupo de amigos de Silvana Rabe...  2017-02-07  cotidiano   \n",
       "\n",
       "     subcategory                                               link  \n",
       "0      ruycastro  http://www1.folha.uol.com.br/colunas/ruycastro...  \n",
       "1  monicabergamo  http://www1.folha.uol.com.br/colunas/monicaber...  \n",
       "2            NaN  http://www1.folha.uol.com.br/esporte/2015/09/1...  \n",
       "3            NaN  http://www1.folha.uol.com.br/mercado/2016/09/1...  \n",
       "4            NaN  http://www1.folha.uol.com.br/cotidiano/2017/07...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding\n",
    "\n",
    "The One-hot encoding will split all the strings based on spaces and will make new columns based on the presence or absence of each unique word on all strings of the column text. This will make an unique binary vector for each datapoint in the column text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words = {'hello': 1, 'world': 7, 'how': 2, 'are': 0, 'you': 8, 'is': 3, 'the': 5, 'says': 4, 'to': 6}\n",
      "Vector of the word \"you\" [[0 0 0 0 0 0 0 0 1]]\n",
      "Vector of the word \"hello\" [[0 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#example\n",
    "\n",
    "example = ['Hello world, how are you?', \n",
    "           'Hello you, how is the world?', \n",
    "           'World says hello to you.']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(example)\n",
    "vocabulary = vectorizer.vocabulary_ #unique words\n",
    "print('Unique words =', vocabulary)\n",
    "\n",
    "you_vector = vectorizer.transform(['you'])\n",
    "print('Vector of the word \"you\"',you_vector.toarray())\n",
    "hello_vector = vectorizer.transform(['hello'])\n",
    "print('Vector of the word \"hello\"',hello_vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uning One-hot encoding one can find out that the vector size is equal to the number of unique words in the data. This will also increase if new words appear in the data. Is there a way to create a vector with the same size to all the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained CBOW Word2Vec model\n",
    "\n",
    "CBOW (Continuous Bag of Words) Word2Vec is a word embedding technique used in natural language processing to represent words as dense, numerical vectors. In simple terms, CBOW tries to predict a target word from its surrounding context words. It takes a group of context words as input and tries to generate the missing target word. The model learns to associate similar words based on their context and creates meaningful vector representations for each word. These word embeddings capture semantic relationships, allowing machines to understand the meaning and context of words. CBOW Word2Vec is popular for tasks like sentiment analysis, language modeling, and information retrieval in NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: C:\\Users\\Gustavo Fortunato\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: gensim in c:\\users\\gustavo fortunato\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\gustavo fortunato\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\gustavo fortunato\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\gustavo fortunato\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (6.3.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CBOW model = http://143.107.183.175:22980/download.php?file=embeddings/wang2vec/cbow_s300.zip\n",
    "%pip install gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('cbow_s300.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.49033e-01  1.26020e-01  2.17628e-01  1.82684e-01  1.65151e-01\n",
      " -1.59660e-01 -2.34411e-01  6.00570e-02  8.03680e-02  2.87578e-01\n",
      " -4.81100e-03 -5.68800e-02  2.15676e-01  8.65540e-02  1.25983e-01\n",
      "  3.36157e-01 -1.83254e-01 -1.18499e-01  1.13010e-02  1.03814e-01\n",
      "  9.37640e-02  2.90178e-01 -1.64395e-01 -1.13300e-02 -1.80676e-01\n",
      " -1.15820e-02  1.08728e-01  1.65898e-01  9.37900e-02  2.66767e-01\n",
      " -1.29890e-02  9.16030e-02  2.21292e-01 -1.36497e-01 -4.26350e-02\n",
      " -1.30038e-01  2.17067e-01 -1.01963e-01 -3.70960e-02  1.42155e-01\n",
      "  3.41109e-01  2.46560e-01  1.27458e-01  5.72360e-02 -1.47962e-01\n",
      " -1.60290e-02  1.86533e-01  7.71550e-02 -3.50024e-01 -4.06085e-01\n",
      "  1.67131e-01 -4.75230e-02  5.13780e-02 -1.28224e-01  1.06580e-02\n",
      " -2.92652e-01  1.40540e-01 -4.57049e-01  1.31094e-01  2.03234e-01\n",
      "  2.94019e-01  7.38370e-02  1.11554e-01 -1.64204e-01 -3.62020e-02\n",
      "  1.29522e-01 -1.28321e-01  1.37502e-01 -7.99200e-03 -5.07100e-03\n",
      " -2.86010e-02 -8.99040e-02  8.82800e-03 -8.27730e-02  6.91940e-02\n",
      " -2.70182e-01  5.47610e-02 -3.06060e-02  6.89880e-02  2.38759e-01\n",
      " -1.41775e-01  2.34763e-01 -2.23853e-01 -2.84994e-01  2.53245e-01\n",
      "  6.77170e-02 -4.39663e-01 -7.00270e-02  6.39150e-02 -9.67100e-02\n",
      " -2.18950e-01 -5.77910e-02 -1.82689e-01 -3.32202e-01 -7.83070e-02\n",
      "  7.74620e-02  8.82920e-02 -4.83618e-01 -1.77812e-01  5.64040e-02\n",
      "  1.50339e-01  8.73000e-02 -1.03121e-01  1.62065e-01  4.57940e-02\n",
      "  9.73590e-02  1.67230e-02  3.00791e-01 -6.49640e-02 -1.95840e-01\n",
      " -4.33790e-02 -9.46810e-02  3.73222e-01 -1.65359e-01  5.58780e-02\n",
      "  1.72660e-02 -3.16048e-01  9.24430e-02 -6.84540e-02 -3.57085e-01\n",
      " -1.69469e-01 -1.14090e-01  9.47230e-02  3.14999e-01  2.12717e-01\n",
      " -2.21540e-02  1.76870e-02  1.58473e-01 -1.39150e-02  1.23610e-02\n",
      " -4.13190e-02 -1.47159e-01 -1.00070e-02  3.41884e-01  1.16999e-01\n",
      " -5.01590e-02  7.88740e-02  6.27940e-02  2.73643e-01  1.46823e-01\n",
      " -1.68857e-01 -1.00014e-01 -5.41060e-02 -3.06130e-02 -8.85920e-02\n",
      " -6.19840e-02  1.21595e-01  1.13775e-01  3.97190e-02 -8.54000e-03\n",
      "  1.05670e-02  1.12375e-01  9.70000e-02  9.05850e-02  1.25026e-01\n",
      " -2.92209e-01  6.81330e-02  4.06070e-02  1.33042e-01 -9.77780e-02\n",
      " -3.26378e-01  9.71420e-02 -5.13600e-02  2.01450e-02  1.20182e-01\n",
      " -2.14210e-02 -1.30884e-01  9.52800e-02 -5.65320e-02 -8.35370e-02\n",
      " -2.53035e-01  9.18650e-02  7.89190e-02 -6.30710e-02 -1.64057e-01\n",
      "  8.31660e-02  1.42698e-01 -2.77053e-01  7.05810e-02 -1.37800e-02\n",
      " -2.74883e-01  3.02011e-01 -8.34330e-02 -1.14381e-01 -2.88826e-01\n",
      "  9.03960e-02  1.94704e-01 -1.57261e-01 -2.58910e-02  1.41321e-01\n",
      " -1.67231e-01 -2.91540e-02  8.03650e-02  1.27378e-01 -1.48120e-01\n",
      "  2.83291e-01 -2.65930e-02  2.15319e-01  3.35030e-02  6.47140e-02\n",
      "  4.20010e-02 -3.85537e-01 -2.67068e-01 -2.77017e-01 -1.82289e-01\n",
      " -1.18735e-01 -2.51480e-01 -1.83783e-01 -2.12362e-01  2.50214e-01\n",
      "  3.96240e-02  2.64830e-02  1.30810e-01 -1.38478e-01 -1.63040e-02\n",
      " -2.55850e-02  2.35141e-01 -8.80540e-02 -9.40650e-02  1.31790e-01\n",
      " -8.33330e-02 -2.40020e-02 -3.38183e-01  8.10370e-02 -1.68933e-01\n",
      "  1.92200e-03  9.34870e-02  6.58130e-02 -1.11925e-01  1.83907e-01\n",
      " -6.54900e-03  4.27730e-02  3.71566e-01 -3.08570e-02  1.99647e-01\n",
      "  1.25516e-01  1.38471e-01 -9.17400e-02 -2.27814e-01  8.27690e-02\n",
      " -2.94581e-01  9.56830e-02 -3.48070e-01  1.02342e-01 -8.05350e-02\n",
      "  2.34290e-02 -4.19860e-02  2.44763e-01  2.37160e-02 -2.23548e-01\n",
      " -9.26800e-03 -4.33650e-02 -1.12413e-01 -4.19178e-01  1.81267e-01\n",
      "  1.03648e-01  2.74945e-01 -9.23560e-02 -4.63300e-02 -2.06314e-01\n",
      "  4.81410e-02  2.64603e-01 -1.17113e-01  1.80097e-01  5.54220e-02\n",
      " -1.27460e-01 -1.60328e-01  1.02289e-01  4.09530e-02  1.25305e-01\n",
      "  1.53398e-01 -2.36950e-02  2.33967e-01  2.30250e-02 -1.40227e-01\n",
      "  3.16349e-01 -1.99592e-01  1.25398e-01  2.72858e-01  1.09793e-01\n",
      " -1.64379e-01  8.63630e-02  1.97445e-01 -2.18180e-02 -1.49784e-01\n",
      " -3.34461e-01 -4.61000e-04  1.92640e-02  2.11149e-01 -2.93349e-01\n",
      "  5.90160e-02  1.25044e-01  7.75570e-02 -2.82863e-01 -3.38890e-02\n",
      " -9.19950e-02 -1.43850e-01  1.45775e-01  1.04246e-01 -2.60548e-01]\n",
      "The vector len is: 300\n",
      "Words most related to \"china\" [('rússia', 0.7320705056190491), ('índia', 0.7241616249084473), ('tailândia', 0.701935887336731), ('indonésia', 0.6860769987106323), ('turquia', 0.6741335988044739), ('malásia', 0.6665689945220947), ('mongólia', 0.6593616008758545), ('manchúria', 0.658184826374054), ('urss', 0.6581669449806213), ('grã-bretanha', 0.6568097472190857)]\n"
     ]
    }
   ],
   "source": [
    "print(model.get_vector('china'))\n",
    "print(\"The vector len is:\", len(model.get_vector('china')))\n",
    "print('Words most related to \"china\"', model.most_similar('china'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the word china returns a relation with the closest countries nearby it, based on the model's vectors. Does that mean one could operate through those vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chile', 0.6781662702560425),\n",
       " ('peru', 0.634803295135498),\n",
       " ('venezuela', 0.6273865699768066),\n",
       " ('equador', 0.6037014126777649),\n",
       " ('bolívia', 0.6017141342163086),\n",
       " ('haiti', 0.5993806719779968),\n",
       " ('méxico', 0.596230685710907),\n",
       " ('paraguai', 0.5957703590393066),\n",
       " ('uruguai', 0.590367317199707),\n",
       " ('japão', 0.5893509387969971)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this will result in the closest words to a list of two words\n",
    "model.most_similar(positive=['brasil','argentina'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('estrelas', 0.46129682660102844),\n",
       " ('névoas', 0.42456939816474915),\n",
       " ('sombras', 0.41976645588874817),\n",
       " ('galáxias', 0.3930572271347046),\n",
       " ('brumas', 0.3914782404899597),\n",
       " ('ondas', 0.3874913454055786),\n",
       " ('poeira', 0.3841949999332428),\n",
       " ('crateras', 0.38199013471603394),\n",
       " ('grimpas', 0.38016703724861145),\n",
       " ('rochas', 0.3793330788612366)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here one tries to find the plural form of the word estrela or star \n",
    "#in english by using nuven (cloud) and nuvens (clouds) as references\n",
    "model.most_similar(positive=['nuvens', 'estrela'], negative=['nuven'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text vetorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Daniel Craig será stormtrooper em novo 'Star Wars', diz ator\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_train.title.loc[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This string contains various elements that are not words and needs to be tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Daniel',\n",
       " 'Craig',\n",
       " 'será',\n",
       " 'stormtrooper',\n",
       " 'em',\n",
       " 'novo',\n",
       " \"'Star\",\n",
       " 'Wars',\n",
       " 'diz',\n",
       " 'ator']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "def tokenizer(text):\n",
    "    alphanumerical_list =[]\n",
    "    for valid_token in nltk.word_tokenize(text):\n",
    "        if valid_token in string.punctuation: continue\n",
    "        alphanumerical_list.append(valid_token)\n",
    "    return alphanumerical_list #numbers and letters only\n",
    "\n",
    "tokenizer(article_train.title.loc[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple vectorization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.017046    0.029228    0.22306101  0.152564   -0.200753   -0.127543\n",
      " -0.146758   -0.04999799 -0.25263101  0.27229601 -0.35945301  0.547111\n",
      " -0.176569    0.1486     -0.252841    0.079226   -0.52047399 -0.007093\n",
      " -0.44340901 -0.121615   -0.23984801 -0.039316   -0.268535    0.34372199\n",
      " -0.06969301  0.23496301 -0.18431701  0.140893    0.099615    0.26310501\n",
      "  0.163359   -0.403374   -0.186685   -0.32900199 -0.56019901  0.247881\n",
      " -0.18952099 -0.19303299 -0.087729   -0.294744    0.08101501  0.22759\n",
      "  0.232398    0.33104201 -0.104798   -0.34920001  0.041333    0.081159\n",
      "  0.189505    0.45870599  0.272421   -0.056138    0.279457   -0.02023801\n",
      "  0.326961   -0.133247   -0.45143999  0.31204    -0.214312    0.257893\n",
      " -0.036405    0.22963601  0.10984901  0.39851099 -0.20284601  0.29613101\n",
      " -0.069412    0.570878   -0.168102    0.02522     0.048971   -0.653547\n",
      " -0.004913   -0.11250401 -0.49022299 -0.18543     0.44946999 -0.092301\n",
      "  0.26825399  0.05362599 -0.225497    0.148662    0.58682597 -0.084838\n",
      " -0.173328   -0.053759   -0.33718699  0.19491701  0.288801   -0.534115\n",
      " -0.36760899 -0.14539201  0.406037   -0.527245    0.18250001 -0.042751\n",
      " -0.416794    0.032635   -0.35422699 -0.103825    0.59644198 -0.366886\n",
      " -0.033544    0.42138201  0.028275    0.220029   -0.319805   -0.13769701\n",
      " -0.39346099  0.046812    0.019395   -0.417365   -0.004198   -0.19284601\n",
      "  0.754985    0.300118   -0.313821    0.135982    0.43342701 -0.06300199\n",
      "  0.274019   -0.004218    0.169457   -0.005891    0.69749602 -0.21940301\n",
      " -0.32435899  0.60351199 -0.12786    -0.53019202 -0.208745   -0.289897\n",
      "  0.016694    0.080253    0.605086    0.22462001 -0.398206   -0.11327299\n",
      "  0.69310501 -0.05538099  0.195223   -0.231216   -0.149792    0.299114\n",
      "  0.159588   -0.006766    0.11637     0.350103    0.54436699 -0.08428\n",
      "  0.42554399 -0.02107099  0.174969   -0.082995   -0.443561    0.026124\n",
      " -0.235195    0.297081    0.45893299 -0.52554802  0.008954   -0.004828\n",
      " -0.22732099 -0.05142     0.239988   -0.310967   -0.168988   -0.09249501\n",
      "  0.145214   -0.54707901 -0.258063   -0.45298399  0.251546    0.167216\n",
      " -0.119703    0.05431401  0.088136   -0.24037399 -0.173953    0.08432999\n",
      " -0.48454899 -0.069199   -0.045346    0.15706601 -0.54067899 -0.26557\n",
      "  0.311245    0.30951    -0.04396999 -0.30020801  0.181455   -0.12860601\n",
      "  0.00111599  0.64530502  0.25590599  0.57547501  0.080723   -0.60767099\n",
      "  0.2915     -0.38584799 -0.167123   -0.15009     0.027046    0.064182\n",
      "  0.007921    0.41308    -0.098647   -0.328196    0.31825    -0.20180101\n",
      "  0.02763999  0.48824601  0.299231   -0.61497501 -0.354963   -0.682394\n",
      "  0.088151    0.03385     0.19263599  0.18103199 -0.10279901  0.15162\n",
      " -0.28098598  0.83046201  0.131392    0.73567003 -0.066534   -0.25608\n",
      "  0.299271    0.009892   -0.229165   -0.73493099 -0.38924299 -0.073556\n",
      " -0.584447    0.20416699 -0.74649101 -0.26934    -0.05578099 -0.058589\n",
      " -0.277597   -0.27533499  0.342219    0.021489    0.251419    0.038382\n",
      "  0.34521501  0.135995   -0.308447    0.13902299  0.063595   -0.117598\n",
      "  0.09407999  0.18364701  0.27958201  0.299869   -0.01231    -0.25011499\n",
      "  0.050748   -0.08361899 -0.072997    0.52241699  0.128572    0.010144\n",
      "  0.137826    0.001522   -0.034324   -0.22148901  0.57424802 -0.26570399\n",
      " -0.019276   -0.364096   -0.350242    0.308368    0.45810401  0.082634\n",
      "  0.016476   -0.26332099 -0.15569301 -0.079638   -0.10411    -0.057249\n",
      "  0.040197   -0.296012   -0.085431    0.03072999 -0.31689501  0.15782399\n",
      " -0.07814899  0.0193     -0.058865    0.224193    0.15475599  0.42073601\n",
      " -0.095801    0.201836   -0.161286   -0.025826    0.341199    0.53848599]\n",
      "The vector len is: 300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sum_of_vectors(alphanumerical_list):\n",
    "    resulting_vector=np.zeros(300) #since the len is 300 and the type is array\n",
    "    for token in alphanumerical_list:\n",
    "        resulting_vector += model.get_vector(token)\n",
    "    return resulting_vector\n",
    "\n",
    "example = tokenizer(\"text example\") \n",
    "resulting_vector = sum_of_vectors(example)\n",
    "\n",
    "print(resulting_vector)\n",
    "print(\"The vector len is:\", len(resulting_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of method works, but it has limitations. One of its main limitations is the vocabulary, if words used in the sum_of_vectors() are not in the vocabulary, the method crashes with the error of word not found. \n",
    "\n",
    "This could be related to the frequency of the word too, since the pre-processing method of Word2Vec applies an UNKNOWN when the word has less than 5 repetitions along the text dataset. All numbers in the strings were also transformed into 0 for units, 00 for dozens, 000 for cents, and further on... (Reference link: https://arxiv.org/abs/1301.3781).\n",
    "\n",
    "Now one needs to fix this, since that information is necesary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.47156008e-01 -1.35562994e-01 -5.09890113e-02  2.28716999e-01\n",
      " -2.11824998e-01 -3.06598997e-01  1.66520005e-01 -4.29335989e-01\n",
      " -7.13000353e-03 -5.56901009e-01  1.78813000e-01 -1.48512000e-01\n",
      " -3.14641990e-01  6.55426025e-01  1.96528994e-01  2.30792001e-01\n",
      " -9.69326997e-01  5.53601235e-03 -8.04177001e-01 -5.03518984e-01\n",
      " -3.70199978e-02 -5.61859952e-02 -3.04392993e-01  7.45470982e-01\n",
      " -5.40238023e-01  2.78465994e-01  5.20579815e-02 -2.75829003e-01\n",
      "  1.45709008e-01  4.80225980e-01 -1.36143997e-01 -2.07818995e-01\n",
      "  7.54090026e-02 -1.23510994e-01 -7.56703012e-01 -4.60499991e-02\n",
      " -4.97524001e-01 -3.79018009e-01  3.59106004e-01 -5.06000007e-02\n",
      "  6.89790128e-02 -2.01459013e-01 -2.92449012e-01 -2.54795002e-01\n",
      "  7.51613986e-01 -1.56610992e-01  1.04944000e-01 -1.57802977e-01\n",
      "  5.90936012e-01  7.49940995e-01  6.13562991e-01  1.31321004e-01\n",
      "  1.20389994e-01  2.15308993e-01  4.76178989e-01 -4.60175000e-01\n",
      "  2.43807994e-01  6.14789002e-01  4.00899008e-01 -1.38468999e-01\n",
      "  2.03336000e-01  8.07155967e-01  7.48920996e-01  1.22736996e-01\n",
      " -4.99621000e-01  2.36207992e-01  4.88696992e-01  1.61118999e-01\n",
      "  6.12249985e-01 -2.49273002e-01  5.08488007e-01 -1.06688800e+00\n",
      "  3.37173991e-01 -6.04448000e-01 -5.47764015e-01 -2.05855001e-01\n",
      " -3.20583000e-01  1.62283994e-01  7.30429944e-02 -1.34349950e-02\n",
      " -7.83364005e-01 -2.28954002e-01  1.14429799e+00 -5.43040037e-02\n",
      " -1.30112000e-01  5.81733990e-01 -9.12496977e-01  9.24819008e-01\n",
      "  5.51024020e-01 -3.47599003e-01  2.36610062e-02 -4.28948997e-01\n",
      " -2.66477000e-01 -5.03174994e-01 -2.93056980e-01  3.81855015e-01\n",
      " -3.63408007e-01 -6.67240016e-01  3.58902998e-01 -1.65965997e-01\n",
      "  7.01431001e-01 -3.08960155e-02  4.92376998e-01 -6.94809947e-02\n",
      "  2.42639929e-02  7.46072009e-01 -3.57415013e-01  4.21985002e-01\n",
      " -7.17482995e-01 -1.11780010e-01  7.74870098e-02 -6.22858997e-01\n",
      "  1.26469995e-01 -8.22844997e-01 -2.42410997e-01  5.15632991e-01\n",
      "  1.55820999e-01 -5.39318005e-01  8.35880011e-01 -1.11379989e-01\n",
      "  3.78117003e-01  1.73317999e-01  1.07245989e-01 -4.06471985e-01\n",
      " -1.00761995e-01 -2.70548016e-01 -3.41655988e-01  6.85667987e-01\n",
      " -4.25496002e-01 -2.35341009e-01 -8.64090007e-02  5.08736994e-01\n",
      " -1.25531995e-01 -4.66926990e-01  4.49463991e-01 -3.28031991e-01\n",
      " -4.14078012e-01 -2.86057005e-01  5.82654994e-01 -2.76780006e-01\n",
      " -1.69159882e-02 -5.15272006e-01  3.84281995e-01 -2.58153003e-01\n",
      " -1.52609998e-01  2.16989983e-02 -9.04409960e-02 -3.01654005e-01\n",
      "  7.08954014e-01 -4.10965012e-01  1.20020002e-01  6.66107006e-01\n",
      " -5.74357986e-01 -6.97844992e-01 -1.77354023e-01  5.11551009e-01\n",
      "  9.28168014e-01 -3.97344990e-01  8.88012990e-01 -3.18711016e-01\n",
      " -1.11262999e-01 -3.38211000e-01 -1.25898001e-01 -2.52602004e-01\n",
      "  6.54733989e-01  1.80958003e-01  2.78262000e-01  2.44101986e-01\n",
      "  2.33385993e-01 -2.90973004e-01  1.22046001e-01  6.16710018e-02\n",
      "  3.78667007e-01 -3.89419002e-01 -9.63794984e-01 -2.28877001e-01\n",
      "  1.34442993e-01 -1.75993994e-01  1.81493998e-01 -1.08329301e+00\n",
      " -3.96181000e-01  2.45979998e-01 -3.82471010e-01 -1.37466028e-01\n",
      "  6.00632008e-01 -1.02765702e+00  5.90726011e-01  4.40520011e-02\n",
      " -3.58228009e-01 -1.89570015e-01 -2.95339003e-01  5.08811997e-01\n",
      " -5.92572987e-01  5.81540026e-01 -1.53506998e-01  1.21623200e+00\n",
      " -1.84427001e-01 -1.27640005e-01  1.26805995e-01  1.46477014e-01\n",
      " -6.07046001e-01 -1.17700147e-02  5.78792010e-01 -4.39792012e-01\n",
      "  7.71530041e-02 -7.26990029e-02  9.53870006e-02 -1.19245016e-01\n",
      "  1.02479696e+00  5.90196981e-01  5.15204980e-01  3.00913993e-01\n",
      "  8.32756013e-01  3.00001237e-04 -2.45450995e-01 -2.05908008e-01\n",
      "  7.60543995e-01 -3.09686985e-01  1.59761996e-01 -1.84663003e-01\n",
      "  4.72609997e-02 -1.03956013e-01 -5.75413011e-01 -3.38090025e-02\n",
      "  6.72727002e-01  7.54022989e-01  5.85720055e-02 -3.80479991e-02\n",
      " -1.01751000e-01 -1.42516002e-01 -1.45972996e-01 -2.87130028e-02\n",
      " -4.73834004e-01  3.36050987e-01  2.70899944e-03  8.61629993e-02\n",
      " -7.27474015e-01  5.54464012e-01  9.81259942e-02 -2.90512003e-01\n",
      "  4.74439971e-02  2.95963006e-01  2.70264000e-01  2.55941002e-01\n",
      "  1.91391993e-01  4.29979001e-01  6.14264004e-01  1.90416001e-01\n",
      "  7.55830996e-01 -3.63619998e-02  5.04569005e-01  4.81027991e-01\n",
      " -1.80335005e-01  4.18259986e-02  4.92081001e-01  7.93219984e-01\n",
      "  1.58029967e-02  1.58900000e-01  9.04203996e-01 -1.98428005e-01\n",
      " -1.18682999e-01  4.40126009e-01  3.94043006e-01  7.08892019e-01\n",
      "  1.61261013e-01 -5.68771008e-01  4.55983013e-01 -2.96930023e-02\n",
      "  9.31475975e-01  3.51601979e-01  9.16580018e-02 -4.88063011e-01\n",
      "  6.92639984e-01  5.55441018e-01 -4.77280002e-02  9.23133988e-01\n",
      "  5.21001033e-03  3.19452992e-01  8.89180154e-02 -1.01132599e+00\n",
      " -8.33728997e-01 -4.09929883e-02  2.60629993e-01 -4.52970006e-02\n",
      " -3.05853002e-01 -3.71623985e-01 -3.11045986e-01  9.04990016e-02\n",
      " -1.14700502e+00 -9.58644986e-01 -5.21200113e-02  1.07584991e-01\n",
      " -4.40325998e-01 -3.49123996e-01 -3.43420003e-02  2.10591011e-01\n",
      " -2.39550002e-01 -2.35368006e-01  3.36973015e-01  1.08086303e+00]\n",
      "The vector len is: 300\n"
     ]
    }
   ],
   "source": [
    "def sum_of_vectors(alphanumerical_list):\n",
    "    resulting_vector=np.zeros(300) #since the len is 300 and the type is array\n",
    "    for token in alphanumerical_list:\n",
    "        try:\n",
    "            resulting_vector += model.get_vector(token)\n",
    "        except KeyError:\n",
    "            if token.isnumeric():\n",
    "               token = '0'*len(token)\n",
    "               resulting_vector += model.get_vector(token)\n",
    "            else:\n",
    "               resulting_vector += model.get_vector('unknown')\n",
    "    return resulting_vector\n",
    "\n",
    "example = tokenizer(\"number 2315 dracula dadouken\") #words with low frequency and numbers\n",
    "resulting_vector = sum_of_vectors(example)\n",
    "\n",
    "print(resulting_vector)\n",
    "print(\"The vector len is:\", len(resulting_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the function work with any kind of alphanumerical character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (90000, 300)\n",
      "test shape (20513, 300)\n"
     ]
    }
   ],
   "source": [
    "def vector_matrix(texts):\n",
    "    x =len(texts)\n",
    "    y= 300\n",
    "    matrix=np.zeros((x,y))\n",
    "    for i in range(x):\n",
    "        alphanumerical = tokenizer(texts.iloc[i])\n",
    "        matrix[i] = sum_of_vectors(alphanumerical)\n",
    "    return matrix\n",
    "\n",
    "train_matrix = vector_matrix(article_train.title)\n",
    "test_matrix = vector_matrix(article_test.title)\n",
    "\n",
    "print('train shape', train_matrix.shape)\n",
    "print('test shape', test_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the texts were vectorized and one is ready to feed the machine model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stabilishing a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General score= 0.29751864671184125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gustavo Fortunato\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Gustavo Fortunato\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.30      1.00      0.46      6103\n",
      "   cotidiano       0.00      0.00      0.00      1698\n",
      "     esporte       0.00      0.00      0.00      4663\n",
      "   ilustrada       0.00      0.00      0.00       131\n",
      "     mercado       0.00      0.00      0.00      5867\n",
      "       mundo       0.00      0.00      0.00      2051\n",
      "\n",
      "    accuracy                           0.30     20513\n",
      "   macro avg       0.05      0.17      0.08     20513\n",
      "weighted avg       0.09      0.30      0.14     20513\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gustavo Fortunato\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "DC = DummyClassifier()\n",
    "DC.fit(train_matrix, article_train.category)\n",
    "dummy_pred = DC.predict(test_matrix)\n",
    "\n",
    "print(f'General score= {DC.score(test_matrix, article_test.category)}')\n",
    "\n",
    "dummy_CR = classification_report(article_test.category, dummy_pred)\n",
    "\n",
    "print(dummy_CR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General score=0.6996538780285673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(max_iter=299) #tested before to avoid warnings\n",
    "LR.fit(train_matrix, article_train.category)\n",
    "print(f'General score={LR.score(test_matrix, article_test.category)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.84      0.68      0.75      6103\n",
      "   cotidiano       0.49      0.63      0.55      1698\n",
      "     esporte       0.83      0.75      0.78      4663\n",
      "   ilustrada       0.08      0.82      0.15       131\n",
      "     mercado       0.79      0.72      0.75      5867\n",
      "       mundo       0.53      0.66      0.59      2051\n",
      "\n",
      "    accuracy                           0.70     20513\n",
      "   macro avg       0.59      0.71      0.60     20513\n",
      "weighted avg       0.76      0.70      0.72     20513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predicted_labels = LR.predict(test_matrix)\n",
    "LR_CR = classification_report(article_test.category, predicted_labels)\n",
    "\n",
    "print(LR_CR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the general score and the classification report the model performed better than the dummy classifier. Based on the weighted average of the metrics the model can still be improved. The weighted average is a standard mean weighted by the amount of data in each category and their metrics. The precision values were good for some categories, but bad for others. The recall values showed that the less accurate results (ilustrada) were the most true ones. This might have happened because of the small amount of data representation by its class(support=131). Similar behavior can be observed in the 'cotidiano' and 'mundo' categories, which had lower data support and also lower metrics. \n",
    "\n",
    "One thing that could be done to increase the metrics could be data augmentation, fine-tuning, or adjusting class weights to address the class imbalance.\n",
    "\n",
    "In summary, the model seems to have performed reasonably well, with an overall accuracy of 0.70. However, it's important to pay attention to the discrepancies in performance across different categories. The model performs well in categories with high precision and recall values, while it struggles in categories with low support and precision values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using SkipGram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gustavo Fortunato\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=299)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=299)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=299)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SkipGram model = http://143.107.183.175:22980/download.php?file=embeddings/fasttext/skip_s300.zip\n",
    "model2 = KeyedVectors.load_word2vec_format('skip_s300.txt')\n",
    "LR2= LogisticRegression(max_iter=299)\n",
    "def sum_of_vectors_model2(alphanumerical_list):\n",
    "    resulting_vector=np.zeros(300) #since the len is 300 and the type is array\n",
    "    for token in alphanumerical_list:\n",
    "        try:\n",
    "            resulting_vector += model2.get_vector(token)\n",
    "        except KeyError:\n",
    "            if token.isnumeric():\n",
    "               token = '0'*len(token)\n",
    "               resulting_vector += model2.get_vector(token)\n",
    "            else:\n",
    "               resulting_vector += model2.get_vector('unknown')\n",
    "    return resulting_vector\n",
    "\n",
    "def vector_matrix_model2(texts):\n",
    "    x =len(texts)\n",
    "    y= 300\n",
    "    matrix=np.zeros((x,y))\n",
    "    for i in range(x):\n",
    "        alphanumerical = tokenizer(texts.iloc[i])\n",
    "        matrix[i] = sum_of_vectors_model2(alphanumerical)\n",
    "    return matrix\n",
    "\n",
    "train_matrix2 = vector_matrix_model2(article_train.title)\n",
    "test_matrix2 = vector_matrix_model2(article_test.title)\n",
    "\n",
    "LR2.fit(train_matrix2, article_train.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.83      0.67      0.75      6103\n",
      "   cotidiano       0.50      0.64      0.56      1698\n",
      "     esporte       0.84      0.75      0.79      4663\n",
      "   ilustrada       0.08      0.80      0.15       131\n",
      "     mercado       0.79      0.73      0.76      5867\n",
      "       mundo       0.54      0.67      0.60      2051\n",
      "\n",
      "    accuracy                           0.71     20513\n",
      "   macro avg       0.60      0.71      0.60     20513\n",
      "weighted avg       0.76      0.71      0.73     20513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels2 = LR2.predict(test_matrix2)\n",
    "CR2 = classification_report(article_test.category, predicted_labels2)\n",
    "\n",
    "print(CR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using SkipGram with the same dimensions as the CBOW used before, the logistic regression performance was a little better. This happened because the SkipGram is a better semantic classification, but slower in training when compared to the CBOW. It is also recommended to use both and see which one performs best for that specific case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The classification created was better than other methods applied to that end. It can be used as a category classifier for the articles, but it still needs improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
