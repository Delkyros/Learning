{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, one will dive deep into the language models and NLP-based data, and create a model that will detect what is the language of the words in the data. The languages that will be detected will be Portuguese-BR, Spanish-LAT, and English-US. The dataset one will be using will be based on the questions made by the stackoverflow community in the languages aforementioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Título</th>\n",
       "      <th>Questão</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Pontuação</th>\n",
       "      <th>Visualizações</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2402</td>\n",
       "      <td>Como fazer hash de senhas de forma segura?</td>\n",
       "      <td>&lt;p&gt;Se eu fizer o &lt;em&gt;&lt;a href=\"http://pt.wikipe...</td>\n",
       "      <td>&lt;hash&gt;&lt;segurança&gt;&lt;senhas&gt;&lt;criptografia&gt;</td>\n",
       "      <td>350</td>\n",
       "      <td>22367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6441</td>\n",
       "      <td>Qual é a diferença entre INNER JOIN e OUTER JOIN?</td>\n",
       "      <td>&lt;p&gt;Qual é a diferença entre &lt;code&gt;INNER JOIN&lt;/...</td>\n",
       "      <td>&lt;sql&gt;&lt;join&gt;</td>\n",
       "      <td>276</td>\n",
       "      <td>176953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>579</td>\n",
       "      <td>Por que não devemos usar funções do tipo mysql_*?</td>\n",
       "      <td>&lt;p&gt;Uma dúvida muito comum é por que devemos pa...</td>\n",
       "      <td>&lt;php&gt;&lt;mysql&gt;</td>\n",
       "      <td>226</td>\n",
       "      <td>9761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2539</td>\n",
       "      <td>As mensagens de erro devem se desculpar?</td>\n",
       "      <td>&lt;p&gt;É comum encontrar uma mensagem de erro que ...</td>\n",
       "      <td>&lt;aplicação-web&gt;&lt;gui&gt;&lt;console&gt;&lt;ux&gt;</td>\n",
       "      <td>214</td>\n",
       "      <td>5075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17501</td>\n",
       "      <td>Qual é a diferença de API, biblioteca e Framew...</td>\n",
       "      <td>&lt;p&gt;Me parecem termos muito próximos e eventual...</td>\n",
       "      <td>&lt;api&gt;&lt;framework&gt;&lt;terminologia&gt;&lt;biblioteca&gt;</td>\n",
       "      <td>193</td>\n",
       "      <td>54191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id                                             Título  \\\n",
       "0   2402         Como fazer hash de senhas de forma segura?   \n",
       "1   6441  Qual é a diferença entre INNER JOIN e OUTER JOIN?   \n",
       "2    579  Por que não devemos usar funções do tipo mysql_*?   \n",
       "3   2539           As mensagens de erro devem se desculpar?   \n",
       "4  17501  Qual é a diferença de API, biblioteca e Framew...   \n",
       "\n",
       "                                             Questão  \\\n",
       "0  <p>Se eu fizer o <em><a href=\"http://pt.wikipe...   \n",
       "1  <p>Qual é a diferença entre <code>INNER JOIN</...   \n",
       "2  <p>Uma dúvida muito comum é por que devemos pa...   \n",
       "3  <p>É comum encontrar uma mensagem de erro que ...   \n",
       "4  <p>Me parecem termos muito próximos e eventual...   \n",
       "\n",
       "                                         Tags  Pontuação  Visualizações  \n",
       "0     <hash><segurança><senhas><criptografia>        350          22367  \n",
       "1                                 <sql><join>        276         176953  \n",
       "2                                <php><mysql>        226           9761  \n",
       "3           <aplicação-web><gui><console><ux>        214           5075  \n",
       "4  <api><framework><terminologia><biblioteca>        193          54191  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the data can be found here: https://github.com/alura-cursos/nlp-modelos-linguagem/tree/8d8385f69247064af5b54452b81a5d9a5cbb9846/dataset\n",
    "import pandas as pd\n",
    "data_pt=pd.read_csv('https://raw.githubusercontent.com/alura-cursos/nlp-modelos-linguagem/8d8385f69247064af5b54452b81a5d9a5cbb9846/dataset/stackoverflow_portugues.csv')\n",
    "data_sp=pd.read_csv('https://raw.githubusercontent.com/alura-cursos/nlp-modelos-linguagem/8d8385f69247064af5b54452b81a5d9a5cbb9846/dataset/stackoverflow_espanhol.csv')\n",
    "data_en=pd.read_csv('https://raw.githubusercontent.com/alura-cursos/nlp-modelos-linguagem/8d8385f69247064af5b54452b81a5d9a5cbb9846/dataset/stackoverflow_ingles.csv')\n",
    "\n",
    "data_pt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Desenvolvi uma página em PHP para uso interno da empresa que trabalho e apenas pouquíssimas pessoas a utilizam. Através dessa página é possível fazer algumas consultas, inserções, alterações e remoções de dados de uma tabela em um banco de dados MySQL, porém eu acredito que meu código em PHP não está protegido contra injeção de código SQL, por exemplo:</p>\n",
      "\n",
      "<pre><code>//----CONSULTA SQL----//\n",
      "$busca = mysql_query ('insert into Produtos (coluna) values(' . $valor . ')');\n",
      "</code></pre>\n",
      "\n",
      "<p>Logo, digamos que o usuário usar a sentença: <code>1); DROP TABLE Produtos;</code> para ao campo <code>valor</code> o comando ficaria: </p>\n",
      "\n",
      "<pre><code>insert into Produtos (coluna) values(1); DROP TABLE Produtos;\n",
      "</code></pre>\n",
      "\n",
      "<p>Ele vai inserir um novo registro cujo o campo <code>coluna</code> será <code>1</code> e logo em seguida ele vai deletar a tabela Produtos.</p>\n",
      "\n",
      "<p>Como posso melhorar meu código para prevenir essa situação?</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_pt.Questão[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here one detects that the code part of the questions has English words. This will potentially affect the model's detection since the question was made on PT-BR. Also, there are other elements that must be removed, since the punctuation and the html parts can affect the language of the question too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using REGEX\n",
    "\n",
    "\n",
    "Regular Expressions (regex) are powerful tools for pattern matching and text manipulation. They consist of a sequence of characters representing a search pattern. With just a few symbols, regex can match complex patterns in strings, making them invaluable in programming, data validation, and text processing tasks.\n",
    "\n",
    "A typical regex includes metacharacters like '.', '*', '+', and '?', allowing for matching any character, repetitions, or optional elements. Character classes like [A-Z] enable matching specific ranges, while '\\d' matches any digit. Anchors like '^' and '$' denote the beginning and end of a line.\n",
    "\n",
    "Regex engines vary, but they generally follow the same principles. When a regex pattern matches a portion of the input text, it returns the position of the match or the matched text itself.\n",
    "\n",
    "Though powerful, regex can be cryptic and prone to misuse. Properly crafted regex requires practice and understanding, but once mastered, they become an invaluable asset for developers and data analysts alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating questions only data from the datasets\n",
    "\n",
    "pt_questions = data_pt.Questão[5]\n",
    "sp_questions = data_sp.Questão[5]\n",
    "en_questions = data_en.Questão[5]\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! One found the first tag, but still, there are plenty more tags for one to find. There must be a way to find another kind of regular expressions in that specific question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<p>Desenvolvi uma página em PHP para uso interno da empresa que trabalho e apenas pouquíssimas pessoas a utilizam. Através dessa página é possível fazer algumas consultas, inserções, alterações e remoções de dados de uma tabela em um banco de dados MySQL, porém eu acredito que meu código em PHP não está protegido contra injeção de código SQL, por exemplo:</p>',\n",
       " '<pre><code>',\n",
       " '</code></pre>',\n",
       " '<p>Logo, digamos que o usuário usar a sentença: <code>1); DROP TABLE Produtos;</code> para ao campo <code>valor</code> o comando ficaria: </p>',\n",
       " '<pre><code>',\n",
       " '</code></pre>',\n",
       " '<p>Ele vai inserir um novo registro cujo o campo <code>coluna</code> será <code>1</code> e logo em seguida ele vai deletar a tabela Produtos.</p>',\n",
       " '<p>Como posso melhorar meu código para prevenir essa situação?</p>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'<.*>', # use .* to find all texts between <>\n",
    "           pt_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the '.', everything between <> will be returned. One needs to use the '?' to stop this from happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<p>',\n",
       " '</p>',\n",
       " '<pre>',\n",
       " '<code>',\n",
       " '</code>',\n",
       " '</pre>',\n",
       " '<p>',\n",
       " '<code>',\n",
       " '</code>',\n",
       " '<code>',\n",
       " '</code>',\n",
       " '</p>',\n",
       " '<pre>',\n",
       " '<code>',\n",
       " '</code>',\n",
       " '</pre>',\n",
       " '<p>',\n",
       " '<code>',\n",
       " '</code>',\n",
       " '<code>',\n",
       " '</code>',\n",
       " '</p>',\n",
       " '<p>',\n",
       " '</p>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'<.*?>', # use .*? to find all texts between <> for once\n",
    "           pt_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  T----E----S----T  Desenvolvi uma página em PHP para uso interno da empresa que trabalho e apenas pouquíssimas pessoas a utilizam. Através dessa página é possível fazer algumas consultas, inserções, alterações e remoções de dados de uma tabela em um banco de dados MySQL, porém eu acredito que meu código em PHP não está protegido contra injeção de código SQL, por exemplo:  T----E----S----T  \n",
      "\n",
      "  T----E----S----T    T----E----S----T  //----CONSULTA SQL----//\n",
      "$busca = mysql_query ('insert into Produtos (coluna) values(' . $valor . ')');\n",
      "  T----E----S----T    T----E----S----T  \n",
      "\n",
      "  T----E----S----T  Logo, digamos que o usuário usar a sentença:   T----E----S----T  1); DROP TABLE Produtos;  T----E----S----T   para ao campo   T----E----S----T  valor  T----E----S----T   o comando ficaria:   T----E----S----T  \n",
      "\n",
      "  T----E----S----T    T----E----S----T  insert into Produtos (coluna) values(1); DROP TABLE Produtos;\n",
      "  T----E----S----T    T----E----S----T  \n",
      "\n",
      "  T----E----S----T  Ele vai inserir um novo registro cujo o campo   T----E----S----T  coluna  T----E----S----T   será   T----E----S----T  1  T----E----S----T   e logo em seguida ele vai deletar a tabela Produtos.  T----E----S----T  \n",
      "\n",
      "  T----E----S----T  Como posso melhorar meu código para prevenir essa situação?  T----E----S----T  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#showing the substitution\n",
    "print(re.sub(r'<.*?>','  T----E----S----T  ',pt_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desenvolvi uma página em PHP para uso interno da empresa que trabalho e apenas pouquíssimas pessoas a utilizam. Através dessa página é possível fazer algumas consultas, inserções, alterações e remoções de dados de uma tabela em um banco de dados MySQL, porém eu acredito que meu código em PHP não está protegido contra injeção de código SQL, por exemplo:\n",
      "\n",
      "//----CONSULTA SQL----//\n",
      "$busca = mysql_query ('insert into Produtos (coluna) values(' . $valor . ')');\n",
      "\n",
      "\n",
      "Logo, digamos que o usuário usar a sentença: 1); DROP TABLE Produtos; para ao campo valor o comando ficaria: \n",
      "\n",
      "insert into Produtos (coluna) values(1); DROP TABLE Produtos;\n",
      "\n",
      "\n",
      "Ele vai inserir um novo registro cujo o campo coluna será 1 e logo em seguida ele vai deletar a tabela Produtos.\n",
      "\n",
      "Como posso melhorar meu código para prevenir essa situação?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#showing the real effect\n",
    "print(re.sub(r'<.*?>','',pt_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The difference between regex and compiled regex\n",
    "\n",
    "Here one will show the speed of regex code and against a compiled regex. This will show the performance of regex, based on the compilation of the metacharacters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6446018999995431"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "setup = \"\"\"import re\"\"\"\n",
    "timeit(\"\"\"re.search(r'70','3423752637846275623847263746238746283746287570')\"\"\",\n",
    "       setup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29342129999713507"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup = \"\"\"import re \n",
    "regex=re.compile(r'70')\"\"\"\n",
    "timeit(\"\"\"regex.search('3423752637846275623847263746238746283746287570')\"\"\",\n",
    "       setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timeit executes 10e6 times the code inside it. The compiled regex is more than twice faster than the re.search function. This happens because each time search is executed the regex is called before the search is done. When the regex is compiled it doesn't need to be called on each search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a function to apply regex on the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Desenvolvi uma página em PHP para uso interno da empresa que trabalho e apenas pouquíssimas pessoas a utilizam. Através dessa página é possível fazer algumas consultas, inserções, alterações e remoções de dados de uma tabela em um banco de dados MySQL, porém eu acredito que meu código em PHP não está protegido contra injeção de código SQL, por exemplo: \n",
      "\n",
      "  //----CONSULTA SQL----//\n",
      "$busca = mysql_query ('insert into Produtos (coluna) values(' . $valor . ')');\n",
      "  \n",
      "\n",
      " Logo, digamos que o usuário usar a sentença:  1); DROP TABLE Produtos;  para ao campo  valor  o comando ficaria:  \n",
      "\n",
      "  insert into Produtos (coluna) values(1); DROP TABLE Produtos;\n",
      "  \n",
      "\n",
      " Ele vai inserir um novo registro cujo o campo  coluna  será  1  e logo em seguida ele vai deletar a tabela Produtos. \n",
      "\n",
      " Como posso melhorar meu código para prevenir essa situação? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remover(texts, regex):\n",
    "    if type(texts) == str:\n",
    "        return regex.sub(' ',texts)\n",
    "    else:\n",
    "        return [regex.sub(' ',text) for text in texts]\n",
    "\n",
    "regex_html = re.compile(r'<.*?>')\n",
    "\n",
    "tagless_question = remover(pt_questions,regex_html)\n",
    "print(tagless_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one will remove the code and not just the html tags. The <code></code> is an HTML tag that marks the beginning and the end of the code in question. So if one removes the HTML first no code tag will exist, the code must be removed before the code tag is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Desenvolvi uma página em PHP para uso interno da empresa que trabalho e apenas pouquíssimas pessoas a utilizam. Através dessa página é possível fazer algumas consultas, inserções, alterações e remoções de dados de uma tabela em um banco de dados MySQL, porém eu acredito que meu código em PHP não está protegido contra injeção de código SQL, por exemplo:</p>\n",
      "\n",
      "<pre> </pre>\n",
      "\n",
      "<p>Logo, digamos que o usuário usar a sentença:   para ao campo   o comando ficaria: </p>\n",
      "\n",
      "<pre> </pre>\n",
      "\n",
      "<p>Ele vai inserir um novo registro cujo o campo   será   e logo em seguida ele vai deletar a tabela Produtos.</p>\n",
      "\n",
      "<p>Como posso melhorar meu código para prevenir essa situação?</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def code_remover(texts, regex):\n",
    "    if type(texts) == str:\n",
    "        return regex.sub(' ',texts)\n",
    "    else:\n",
    "        return [regex.sub(' ',text) for text in texts]\n",
    "    \n",
    "#now the regex will find either( a linebreak '\\n' or '|'  anything else except the linebreak '.')\n",
    "regex_code = re.compile(r'<code>(.|(\\n))*?</code>')\n",
    "\n",
    "codeless_question = remover(pt_questions,regex_code)\n",
    "print(codeless_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Desenvolvi uma página em PHP para uso interno da empresa que trabalho e apenas pouquíssimas pessoas a utilizam. Através dessa página é possível fazer algumas consultas, inserções, alterações e remoções de dados de uma tabela em um banco de dados MySQL, porém eu acredito que meu código em PHP não está protegido contra injeção de código SQL, por exemplo: \n",
      "\n",
      "   \n",
      "\n",
      " Logo, digamos que o usuário usar a sentença:   para ao campo   o comando ficaria:  \n",
      "\n",
      "   \n",
      "\n",
      " Ele vai inserir um novo registro cujo o campo   será   e logo em seguida ele vai deletar a tabela Produtos. \n",
      "\n",
      " Como posso melhorar meu código para prevenir essa situação? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def html_remover(texts, regex):\n",
    "    if type(texts) == str:\n",
    "        return regex.sub(' ',texts)\n",
    "    else:\n",
    "        return [regex.sub(' ',text) for text in texts]\n",
    "\n",
    "regex_html = re.compile(r'<.*?>')\n",
    "\n",
    "tagless_question = html_remover(codeless_question,regex_html)\n",
    "print(tagless_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the question is clear of any text element that could affect the language detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_questions_ = code_remover(data_pt.Questão, regex_code)\n",
    "pt_questions_clean = html_remover(pt_questions_, regex_html)\n",
    "\n",
    "sp_questions_ = code_remover(data_sp.Questão, regex_code)\n",
    "sp_questions_clean = html_remover(sp_questions_, regex_html)\n",
    "\n",
    "en_questions_ = code_remover(data_en.Questão, regex_code)\n",
    "en_questions_clean = html_remover(en_questions_, regex_html)\n",
    "\n",
    "data_pt['clean_questions'] = pt_questions_clean\n",
    "data_sp['clean_questions'] = sp_questions_clean\n",
    "data_en['clean_questions'] = en_questions_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Qual é a diferença entre   e  ? Podem me dar alguns exemplos? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_pt['clean_questions'] [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Siempre he visto que en   hay: \n",
      "\n",
      " \n",
      " asignaciones   \n",
      " comparaciones   y   \n",
      " \n",
      "\n",
      " Creo entender que   hace algo parecido a comparar el valor de la variable y el   también compara el tipo (como un equals de java).  \n",
      "\n",
      "  ¿Alguien podría confirmarme este punto y extenderlo? . Soy javero y el no tipado de javascript a veces me encanta y otras lo odio. \n",
      "\n",
      " \n",
      "\n",
      " ¿Cuál es la manera correcta en javascript de comparar  ,   y otros valores por defecto?  \n",
      "\n",
      "   \n",
      "\n",
      " ¿    se usa como cadena de texto o como palabra clave? ¿Cual de las siguientes comparaciones es la correcta para un elemento   sin  ? (por ejemplo un label sin contenido) \n",
      "\n",
      "   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_sp['clean_questions'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the dataset is clean. The thing is that Spanish has a unique question mark that must be removed since this will bias the model classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I accidentally committed the wrong files to  Git , but I haven't pushed the commit to the server yet. \n",
      "\n",
      "  How can I undo those commits from the local repository?   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_en['clean_questions'] [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dracula  Ah   sarcasm   For what profit is it to  a  \n",
      "man if he gains the world  and \n",
      "\n",
      "loses his own soul    Matthew 16 26 I believe \n"
     ]
    }
   ],
   "source": [
    "# \\w will remove everything that is not alpha numeric\n",
    "# \\s will allow the spaces and linebreaks to be kept\n",
    "example = \"Dracula: Ah...sarcasm. 'For what profit is it to [a] \\nman if he gains the world, and \\n\\nloses his own soul'?. Matthew 16:26 I believe.\"\n",
    "\n",
    "regex_punctuation = re.compile(r'[^\\w\\s]')\n",
    "print(remover(example, regex_punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dracula  ah   sarcasm   for what profit is it to  a  \n",
      "man if he gains the world  and \n",
      "\n",
      "loses his own soul    matthew 16 26 i believe \n"
     ]
    }
   ],
   "source": [
    "example = remover(example, regex_punctuation)\n",
    "\n",
    "data_pt['clean_questions'] = remover(data_pt['clean_questions'], regex_punctuation)\n",
    "data_sp['clean_questions'] = remover(data_sp['clean_questions'], regex_punctuation)\n",
    "data_en['clean_questions'] = remover(data_en['clean_questions'], regex_punctuation)\n",
    "\n",
    "def case_remover(texts):\n",
    "    if type(texts) == str:\n",
    "        return texts.lower()\n",
    "    else:\n",
    "        return [text.lower() for text in texts]\n",
    "\n",
    "\n",
    "data_pt['clean_questions'] = case_remover(data_pt['clean_questions'])\n",
    "data_sp['clean_questions'] = case_remover(data_sp['clean_questions'])\n",
    "data_en['clean_questions'] = case_remover(data_en['clean_questions'])\n",
    "\n",
    "print(case_remover(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dracula  ah   sarcasm   for what profit is it to  a  \n",
      "man if he gains the world  and \n",
      "\n",
      "loses his own soul    matthew     i believe \n"
     ]
    }
   ],
   "source": [
    "# \\d to remove digits\\numbers\n",
    "\n",
    "example= case_remover(example)\n",
    "regex_numbers = re.compile(r'\\d+')\n",
    "\n",
    "data_pt['clean_questions'] = remover(data_pt['clean_questions'],regex_numbers)\n",
    "data_sp['clean_questions'] = remover(data_sp['clean_questions'],regex_numbers)\n",
    "data_en['clean_questions'] = remover(data_en['clean_questions'],regex_numbers)\n",
    "\n",
    "print(remover(example, regex_numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space and linebreak adjustments\n",
    "\n",
    "Some questions could have '  ' and \\n\\n. These can affect the model's \"reading ability\", since both could be a word to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dracula  ah   sarcasm   for what profit is it to  a   man if he gains the world  and   loses his own soul    matthew     i believe \n",
      "dracula ah sarcasm for what profit is it to a man if he gains the world and loses his own soul matthew i believe \n"
     ]
    }
   ],
   "source": [
    "\n",
    "example= remover(example, regex_numbers)\n",
    "regex_linebreak = re.compile(r'(\\n)')\n",
    "data_pt['clean_questions'] = remover(data_pt['clean_questions'],regex_linebreak)\n",
    "data_sp['clean_questions'] = remover(data_sp['clean_questions'],regex_linebreak)\n",
    "data_en['clean_questions'] = remover(data_en['clean_questions'],regex_linebreak)\n",
    "\n",
    "print(remover(example, regex_linebreak))\n",
    "example = remover(example, regex_linebreak)\n",
    "\n",
    "regex_spaces = re.compile(r' {2,}')\n",
    "\n",
    "data_pt['clean_questions'] = remover(data_pt['clean_questions'],regex_spaces)\n",
    "data_sp['clean_questions'] = remover(data_sp['clean_questions'],regex_spaces)\n",
    "data_en['clean_questions'] = remover(data_en['clean_questions'],regex_spaces)\n",
    "\n",
    "print(re.sub(r' {2,}', ' ',example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " se eu fizer o hash de senhas antes de armazená las em meu banco de dados é suficiente para evitar que elas sejam recuperadas por alguém estou falando apenas da recuperação diretamente do banco de dados e não qualquer outro tipo de ataque como força bruta na página de login da aplicação keylogger no cliente e criptoanálise rubberhose qualquer forma de hash não vai impedir esses ataques tenho preocupação em dificultar ou até impossibilitar a obtenção das senhas originais caso o banco de dados seja comprometido como dar maior garantia de segurança neste aspecto quais preocupações adicionais evitariam o acesso às senhas existem formas melhores de fazer esse hash \n",
      " las sentencias dinámicas son sentencias sql que se crean como cadenas de texto strings y en las que se insertan concatenan valores obtenidos de alguna fuente normalmente proveniente del usuario lo que puede hacer que sean vulnerables a inyección sql si no se sanean las entradas como por ejemplo eso es un ejemplo de una vulnerabilidad grave en la seguridad de una aplicación web o no porque si el usuario introdujese un valor como nos encontraríamos con que la sentencia ejecutada sería y se eliminaría la tabla usuarios con todos los datos contenidos en ella cómo puedo evitar que la inyección sql ocurra en php \n",
      " here is a piece of c code that seems very peculiar for some strange reason sorting the data miraculously makes the code almost six times faster without the code runs in seconds with the sorted data the code runs in seconds initially i thought this might be just a language or compiler anomaly so i tried it in java with a somewhat similar but less extreme result my first thought was that sorting brings the data into the cache but then i thought how silly that is because the array was just generated what is going on why is it faster to process a sorted array than an unsorted array the code is summing up some independent terms and the order should not matter \n"
     ]
    }
   ],
   "source": [
    "#just to make sure everything is ok\n",
    "print(data_pt['clean_questions'][0])\n",
    "print(data_sp['clean_questions'][0])\n",
    "print(data_en['clean_questions'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready to be modelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Char insertion\n",
    "\n",
    "To detect a language one must calculate the odds of a certain letter being followed by another letter. The model will be based on bigrams made with that language words. Those bigrams will have the odds about letter positions relating to each word. This will help the model understand any new word and the current words it already knows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d', 'r'), ('r', 'a'), ('a', 'c'), ('c', 'u'), ('u', 'l'), ('l', 'a'), ('a', ' '), (' ', 'a'), ('a', 'h'), ('h', ' '), (' ', 's'), ('s', 'a'), ('a', 'r'), ('r', 'c'), ('c', 'a'), ('a', 's'), ('s', 'm'), ('m', ' '), (' ', 'f'), ('f', 'o'), ('o', 'r'), ('r', ' '), (' ', 'w'), ('w', 'h'), ('h', 'a'), ('a', 't'), ('t', ' '), (' ', 'p'), ('p', 'r'), ('r', 'o'), ('o', 'f'), ('f', 'i'), ('i', 't'), ('t', ' '), (' ', 'i'), ('i', 's'), ('s', ' '), (' ', 'i'), ('i', 't'), ('t', ' '), (' ', 't'), ('t', 'o'), ('o', ' '), (' ', 'a'), ('a', ' '), (' ', 'm'), ('m', 'a'), ('a', 'n'), ('n', ' '), (' ', 'i'), ('i', 'f'), ('f', ' '), (' ', 'h'), ('h', 'e'), ('e', ' '), (' ', 'g'), ('g', 'a'), ('a', 'i'), ('i', 'n'), ('n', 's'), ('s', ' '), (' ', 't'), ('t', 'h'), ('h', 'e'), ('e', ' '), (' ', 'w'), ('w', 'o'), ('o', 'r'), ('r', 'l'), ('l', 'd'), ('d', ' '), (' ', 'a'), ('a', 'n'), ('n', 'd'), ('d', ' '), (' ', 'l'), ('l', 'o'), ('o', 's'), ('s', 'e'), ('e', 's'), ('s', ' '), (' ', 'h'), ('h', 'i'), ('i', 's'), ('s', ' '), (' ', 'o'), ('o', 'w'), ('w', 'n'), ('n', ' '), (' ', 's'), ('s', 'o'), ('o', 'u'), ('u', 'l'), ('l', ' '), (' ', 'm'), ('m', 'a'), ('a', 't'), ('t', 't'), ('t', 'h'), ('h', 'e'), ('e', 'w'), ('w', ' '), (' ', 'i'), ('i', ' '), (' ', 'b'), ('b', 'e'), ('e', 'l'), ('l', 'i'), ('i', 'e'), ('e', 'v'), ('v', 'e'), ('e', ' ')]\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "\n",
    "from nltk import bigrams\n",
    "example = re.sub(r' {2,}', ' ',example)\n",
    "bigrams(example)\n",
    "print(list(bigrams(example)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each letter was connected to the next or previous letter. This will allow the model to calculate the odds of that letter being more frequent in certain languages than others. However, this could still bias the model since the first letter of the comments could be the most probable word in any new sentence. The starting word must be defined in any sentence to prevent this bias from happening. This process is called padding with a fake char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'd'), ('d', 'r'), ('r', 'a'), ('a', 'c'), ('c', 'u'), ('u', 'l'), ('l', 'a'), ('a', ' '), (' ', 'a'), ('a', 'h'), ('h', ' '), (' ', 's'), ('s', 'a'), ('a', 'r'), ('r', 'c'), ('c', 'a'), ('a', 's'), ('s', 'm'), ('m', ' '), (' ', 'f'), ('f', 'o'), ('o', 'r'), ('r', ' '), (' ', 'w'), ('w', 'h'), ('h', 'a'), ('a', 't'), ('t', ' '), (' ', 'p'), ('p', 'r'), ('r', 'o'), ('o', 'f'), ('f', 'i'), ('i', 't'), ('t', ' '), (' ', 'i'), ('i', 's'), ('s', ' '), (' ', 'i'), ('i', 't'), ('t', ' '), (' ', 't'), ('t', 'o'), ('o', ' '), (' ', 'a'), ('a', ' '), (' ', 'm'), ('m', 'a'), ('a', 'n'), ('n', ' '), (' ', 'i'), ('i', 'f'), ('f', ' '), (' ', 'h'), ('h', 'e'), ('e', ' '), (' ', 'g'), ('g', 'a'), ('a', 'i'), ('i', 'n'), ('n', 's'), ('s', ' '), (' ', 't'), ('t', 'h'), ('h', 'e'), ('e', ' '), (' ', 'w'), ('w', 'o'), ('o', 'r'), ('r', 'l'), ('l', 'd'), ('d', ' '), (' ', 'a'), ('a', 'n'), ('n', 'd'), ('d', ' '), (' ', 'l'), ('l', 'o'), ('o', 's'), ('s', 'e'), ('e', 's'), ('s', ' '), (' ', 'h'), ('h', 'i'), ('i', 's'), ('s', ' '), (' ', 'o'), ('o', 'w'), ('w', 'n'), ('n', ' '), (' ', 's'), ('s', 'o'), ('o', 'u'), ('u', 'l'), ('l', ' '), (' ', 'm'), ('m', 'a'), ('a', 't'), ('t', 't'), ('t', 'h'), ('h', 'e'), ('e', 'w'), ('w', ' '), (' ', 'i'), ('i', ' '), (' ', 'b'), ('b', 'e'), ('e', 'l'), ('l', 'i'), ('i', 'e'), ('e', 'v'), ('v', 'e'), ('e', ' '), (' ', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "print(list(bigrams(pad_both_ends(example, n=2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now an HTML mark, appears on the starting and ending word of the sentence. With this, the model will know that is the most common letter that starts sentences and ends them.\n",
    "\n",
    "The process above was made just to show what bigrams are, the real faster way to use them will be made below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language column definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pt['Language']='portuguese'\n",
    "data_sp['Language']='spanish'\n",
    "data_en['Language']='english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pt_train, pt_test = train_test_split(data_pt.clean_questions, test_size=0.2,\n",
    "                                     random_state=123)\n",
    "sp_train, sp_test = train_test_split(data_sp.clean_questions, test_size=0.2,\n",
    "                                     random_state=123)\n",
    "en_train, en_test = train_test_split(data_en.clean_questions, test_size=0.2,\n",
    "                                     random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pt_questions = ' '.join(pt_train)\n",
    "all_sp_questions = ' '.join(sp_train)\n",
    "all_en_questions = ' '.join(en_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "all_pt_words = WhitespaceTokenizer().tokenize(all_pt_questions)\n",
    "all_sp_words = WhitespaceTokenizer().tokenize(all_sp_questions)\n",
    "all_en_words = WhitespaceTokenizer().tokenize(all_en_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying bigram by pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s>',)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "pt_train_bigram, pt_vocab = padded_everygram_pipeline(2, all_pt_words)\n",
    "sp_train_bigram, sp_vocab = padded_everygram_pipeline(2, all_sp_words)\n",
    "en_train_bigram, en_vocab = padded_everygram_pipeline(2, all_en_words)\n",
    "#those variables are generator type\n",
    "\n",
    "next(next(pt_train_bigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MLE\n",
    "\n",
    "Maximum Likelihood Estimator (MLE) is a fundamental statistical technique that can be applied in Natural Language Processing (NLP). It aims to estimate parameters for language models by maximizing the likelihood of observing the given data. In NLP, MLE plays a crucial role in tasks like language modeling, part-of-speech tagging, and sentiment analysis. By finding the most probable parameters, MLE allows one to generate more accurate predictions and improve NLP models' performance. This allows the model to comprehend, generate, and process human language effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "model_pt = MLE(2)\n",
    "model_pt.fit(pt_train_bigram, pt_vocab)\n",
    "\n",
    "model_sp = MLE(2)\n",
    "model_sp.fit(sp_train_bigram, sp_vocab)\n",
    "\n",
    "model_en = MLE(2)\n",
    "model_en.fit(en_train_bigram, en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', '</s>', 's', 's', 't', 'h', 'y', '</s>']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generating a word based on the model's knowledge\n",
    "model_en.generate(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the afore-generated word, what is the most probable char after the letter o?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('w', 509), ('</s>', 1257), ('u', 500), ('g', 65), ('l', 212), ('m', 472), ('n', 1069), ('r', 895), ('e', 87), ('d', 237), ('b', 98), ('f', 375), ('o', 144), ('p', 202), ('t', 303), ('s', 181), ('k', 62), ('v', 103), ('i', 80), ('c', 99), ('x', 21), ('z', 2), ('a', 35), ('j', 19), ('h', 4), ('y', 7)])\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import NgramCounter\n",
    "print(model_en.counts[['o']].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word does not make any sense and the next word probability also don't. Since the aim of this notebook is not text generation, this was only used to show that the model requires more data to generate a better word.\n",
    "\n",
    "Now one will use the models to see what is the most probable language from the example used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('<s>', 'd'), ('d', 'r'), ('r', 'a'), ('a', 'c'), ('c', 'u'), ('u', 'l'), ('l', 'a'), ('a', '</s>')], [('<s>', 'a'), ('a', 'h'), ('h', '</s>')], [('<s>', 's'), ('s', 'a'), ('a', 'r'), ('r', 'c'), ('c', 'a'), ('a', 's'), ('s', 'm'), ('m', '</s>')], [('<s>', 'f'), ('f', 'o'), ('o', 'r'), ('r', '</s>')], [('<s>', 'w'), ('w', 'h'), ('h', 'a'), ('a', 't'), ('t', '</s>')], [('<s>', 'p'), ('p', 'r'), ('r', 'o'), ('o', 'f'), ('f', 'i'), ('i', 't'), ('t', '</s>')], [('<s>', 'i'), ('i', 's'), ('s', '</s>')], [('<s>', 'i'), ('i', 't'), ('t', '</s>')], [('<s>', 't'), ('t', 'o'), ('o', '</s>')], [('<s>', 'a'), ('a', '</s>')], [('<s>', 'm'), ('m', 'a'), ('a', 'n'), ('n', '</s>')], [('<s>', 'i'), ('i', 'f'), ('f', '</s>')], [('<s>', 'h'), ('h', 'e'), ('e', '</s>')], [('<s>', 'g'), ('g', 'a'), ('a', 'i'), ('i', 'n'), ('n', 's'), ('s', '</s>')], [('<s>', 't'), ('t', 'h'), ('h', 'e'), ('e', '</s>')], [('<s>', 'w'), ('w', 'o'), ('o', 'r'), ('r', 'l'), ('l', 'd'), ('d', '</s>')], [('<s>', 'a'), ('a', 'n'), ('n', 'd'), ('d', '</s>')], [('<s>', 'l'), ('l', 'o'), ('o', 's'), ('s', 'e'), ('e', 's'), ('s', '</s>')], [('<s>', 'h'), ('h', 'i'), ('i', 's'), ('s', '</s>')], [('<s>', 'o'), ('o', 'w'), ('w', 'n'), ('n', '</s>')], [('<s>', 's'), ('s', 'o'), ('o', 'u'), ('u', 'l'), ('l', '</s>')], [('<s>', 'm'), ('m', 'a'), ('a', 't'), ('t', 't'), ('t', 'h'), ('h', 'e'), ('e', 'w'), ('w', '</s>')], [('<s>', 'i'), ('i', '</s>')], [('<s>', 'b'), ('b', 'e'), ('e', 'l'), ('l', 'i'), ('i', 'e'), ('e', 'v'), ('v', 'e'), ('e', '</s>')]]\n"
     ]
    }
   ],
   "source": [
    "example = WhitespaceTokenizer().tokenize(example)\n",
    "example = [list(pad_both_ends(word, n=2)) for word in example]\n",
    "example = [list(bigrams(word)) for word in example]\n",
    "\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the model one will use perplexity. The lower the value the closer the word is to the model's language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portguese model's perplexicy:\n",
      "Word 'for'=8.868771423894772\n",
      "Word 'what'=36.397341865405124\n",
      "Word 'profit'=15.80223293935864\n",
      "Spanish model's perplexicy:\n",
      "Word 'for'=10.71834844408005\n",
      "Word 'what'=26.2454558218365\n",
      "Word 'profit'=17.594605819820316\n",
      "English model's perplexicy:\n",
      "Word 'for'=8.556057461847379\n",
      "Word 'what'=6.243240369180808\n",
      "Word 'profit'=9.638932107008605\n"
     ]
    }
   ],
   "source": [
    "print(f\"Portguese model's perplexicy:\\nWord 'for'={model_pt.perplexity(example[3])}\\nWord 'what'={model_pt.perplexity(example[4])}\\nWord 'profit'={model_pt.perplexity(example[5])}\")\n",
    "print(f\"Spanish model's perplexicy:\\nWord 'for'={model_sp.perplexity(example[3])}\\nWord 'what'={model_sp.perplexity(example[4])}\\nWord 'profit'={model_sp.perplexity(example[5])}\")\n",
    "print(f\"English model's perplexicy:\\nWord 'for'={model_en.perplexity(example[3])}\\nWord 'what'={model_en.perplexity(example[4])}\\nWord 'profit'={model_en.perplexity(example[5])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words used were: for, what, and profit, and the results are right, those words are in English and the lowest values of perplexity are found in the English model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some automations and testing the model\n",
    "\n",
    "This will be the last part when one creates functions to automate some of the processes made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLE_train(text_list):\n",
    "    all_questions = ' '.join(text_list)\n",
    "    all_words = WhitespaceTokenizer().tokenize(all_questions)\n",
    "    bigrams, vocabulary = padded_everygram_pipeline(2, all_words)\n",
    "    model = MLE(2)\n",
    "    model.fit(bigrams, vocabulary)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pt = MLE_train(pt_train)\n",
    "model_sp = MLE_train(sp_train)\n",
    "model_en = MLE_train(en_train)\n",
    "\n",
    "def perplexity(model, text):\n",
    "    perplexity = 0\n",
    "    words = WhitespaceTokenizer().tokenize(text)\n",
    "    words_fakechar = [list(pad_both_ends(word, n=2)) for word in words]\n",
    "    words_bigrams = [list(bigrams(word)) for word in words_fakechar]\n",
    "    for word in words_bigrams:\n",
    "        perplexity += model.perplexity(word)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the models on the pt_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010.750019400955\n",
      "inf\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "print(perplexity(model_pt,pt_test.iloc[0]))\n",
    "print(perplexity(model_sp,pt_test.iloc[0]))\n",
    "print(perplexity(model_en,pt_test.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the models on the sp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "711.8401975455689\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "print(perplexity(model_pt,sp_test.iloc[0]))\n",
    "print(perplexity(model_sp,sp_test.iloc[0]))\n",
    "print(perplexity(model_en,sp_test.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the models on the en_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389.8361353691403\n",
      "374.2808748500097\n",
      "182.14359532129498\n"
     ]
    }
   ],
   "source": [
    "print(perplexity(model_pt,en_test.iloc[0]))\n",
    "print(perplexity(model_sp,en_test.iloc[0]))\n",
    "print(perplexity(model_en,en_test.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the test results one confirms that the models are detecting the language correctly from the first test sample. Sometimes the perplexity could be infinite. This happens because of the probability is = 0, so perplexity is defined by 1/P or 1/0 = inf.\n",
    "\n",
    "This doesn't mean that the model is very accurate, instead it could mean that the test sample used has words that couldn't be found in the training set. One needs to be alert about that and always use another model to make sure of its conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Laplace model\n",
    "\n",
    "This model will allow one to smooth the results increasing the probabilities in a way that the inf values will not appear anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on portuguese sample test \n",
      " pt_model  2011.2813028930555\n",
      "sp_model  3396.228242347294\n",
      "en_model  5814.344086715258 \n",
      "\n",
      "Testing on spanish sample test \n",
      " pt_model  1021.5558410607784\n",
      "sp_model  718.0246771504738\n",
      "en_model  1204.7546685979514 \n",
      "\n",
      "Testing on english sample test \n",
      " pt_model  398.23167300320694\n",
      "sp_model  378.47899379671964\n",
      "en_model  182.95105404626915\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import Laplace\n",
    "\n",
    "def laplace_train(text_list):\n",
    "    all_questions = ' '.join(text_list)\n",
    "    all_words = WhitespaceTokenizer().tokenize(all_questions)\n",
    "    bigrams, vocabulary = padded_everygram_pipeline(2, all_words)\n",
    "    model = Laplace(2)\n",
    "    model.fit(bigrams, vocabulary)\n",
    "    return model\n",
    "\n",
    "model_pt = laplace_train(pt_train)\n",
    "model_sp = laplace_train(sp_train)\n",
    "model_en = laplace_train(en_train)\n",
    "\n",
    "def perplexity(model, text):\n",
    "    perplexity = 0\n",
    "    words = WhitespaceTokenizer().tokenize(text)\n",
    "    words_fakechar = [list(pad_both_ends(word, n=2)) for word in words]\n",
    "    words_bigrams = [list(bigrams(word)) for word in words_fakechar]\n",
    "    for word in words_bigrams:\n",
    "        perplexity += model.perplexity(word)\n",
    "    return perplexity\n",
    "\n",
    "print('Testing on portuguese sample test','\\n','pt_model ',perplexity(model_pt,pt_test.iloc[0]))\n",
    "print('sp_model ', perplexity(model_sp,pt_test.iloc[0]))\n",
    "print('en_model ',perplexity(model_en,pt_test.iloc[0]),'\\n')\n",
    "\n",
    "print('Testing on spanish sample test','\\n','pt_model ',perplexity(model_pt,sp_test.iloc[0]))\n",
    "print('sp_model ',perplexity(model_sp,sp_test.iloc[0]))\n",
    "print('en_model ',perplexity(model_en,sp_test.iloc[0]),'\\n')\n",
    "\n",
    "print('Testing on english sample test','\\n','pt_model ',perplexity(model_pt,en_test.iloc[0]))\n",
    "print('sp_model ',perplexity(model_sp,en_test.iloc[0]))\n",
    "print('en_model ',perplexity(model_en,en_test.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function that knows the text language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC pt model 100.0\n",
      "ACC sp model 96.0\n",
      "ACC en model 99.0\n"
     ]
    }
   ],
   "source": [
    "def know_language(text_list):\n",
    "    language  = []\n",
    "    for text in text_list:\n",
    "        pt = perplexity(model_pt,text)\n",
    "        sp = perplexity(model_sp,text)\n",
    "        en = perplexity(model_en,text)\n",
    "        if en >= pt <= sp:\n",
    "            language.append('Portuguese')\n",
    "        elif pt> en < sp:\n",
    "            language.append('English')\n",
    "        else:\n",
    "            language.append('Spanish')\n",
    "    return language\n",
    "\n",
    "pt_results = know_language(pt_test)\n",
    "print('ACC pt model',(pt_results.count('Portuguese')/len(sp_test))*100)\n",
    "sp_results = know_language(sp_test)\n",
    "print('ACC sp model',(sp_results.count('Spanish')/len(sp_test))*100)\n",
    "en_results = know_language(en_test)\n",
    "print('ACC en model',(en_results.count('English')/len(sp_test))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the pt model is perfect, but the others are not. This doesn't disqualify the work done, since one had a limited dataset. For NLP and language detection, the datasets used are huge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
